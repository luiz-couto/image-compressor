{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import fftpack\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(matrix, num):\n",
    "    sections = []\n",
    "    for i in range(0, matrix.shape[0], num):\n",
    "        for j in range(0, matrix.shape[1], num):\n",
    "            section = matrix[i:i+num, j:j+num]\n",
    "            sections.append(section)\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_2d_idct(coefficients):\n",
    "    \"\"\" Get 2D Inverse Cosine Transform of Image\n",
    "    \"\"\"\n",
    "    return fftpack.idct(fftpack.idct(coefficients.T, norm='ortho').T, norm='ortho')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"landscape.jpg\")\n",
    "img_width = img.shape[1]\n",
    "img_height = img.shape[0]\n",
    "len(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressor de Imagens\n",
    "O método utilizado nesse trabalho seguirá as diretrizes do método JPEG para imagens cromáticas (com cores). Será um método com perdas. Esse método pode ser dividido em 5 etapas:\n",
    "1. Conversão do espaço de cores\n",
    "2. Downsampling da chrominance\n",
    "3. Aplicação da DCT\n",
    "4. Quantização\n",
    "5. Codificação de Símbolos utilzando Huffman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão do Espaço de Cores\n",
    "Cada pixel de uma imagem colorida é dividida em 3 valores: R(red), G(green), B(blue). No método JPEG, esses valores são convertidos para Y(Luminance) Cb(blue chrominance) Cr(red chrominance). Isso é feito pois o olho humano é capaz de detectar brilho e luminâcia melhor do que detectar cores. Ao separar as cores desse modo, podemos, no próximo passo (Chrominance Downsampling) descartar algumas informações de Cb e Cr, de forma a já diminuir o tamanho necessário para cada pixel enquanto mantemos Y(que se refere ao brilho) intacto. Nesse caso, exploramos a redundânica psicovisual ao remover informações que não são facilmente detectadas, e mantendo aquelas que conseguimos perceber melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRGBToYCbCr(imgInRGB):\n",
    "    YCbCr_img = cv2.cvtColor(imgInRGB, cv2.COLOR_RGB2YCrCb)\n",
    "    Y, Cr, Cb = cv2.split(YCbCr_img)\n",
    "    return Y, Cb, Cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y, _Cb, _Cr = convertRGBToYCbCr(img)\n",
    "len(_Cb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrominance Downsampling\n",
    "Para realizar o downsampling, vamos pegar as matrizes referentes à Cb e Cr e então dividi-las em blocos de 2x2 pixels. Para cada bloco, fazemos a média dos valores dos 4 pixels, e então substituimos o bloco inteiro por apenas o valor calculado. Eliminamos, assim, grande parte da informação que nossos olhos mal conseguem perceber. Repare que desse modo diminuimos as matrizes Cb e Cr em 1/4 da imagem da original. Porém, mantivemos a matriz Y no seu tamanho original, que possui os valores relacionados ao brilho e luminânica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsamplingChrominance(chrominanceMatrix):\n",
    "    pixelSections = split(chrominanceMatrix, 2)\n",
    "\n",
    "    new_matrix = np.zeros((round(img_height/2), round(img_width/2)))\n",
    "    print(new_matrix.shape)\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0, round(img_height/2)):\n",
    "        for j in range(0, round(img_width/2)):\n",
    "            pixelSection = pixelSections[counter]\n",
    "            new_matrix[i][j] = int(round(pixelSection.sum() / 4))\n",
    "            counter += 1\n",
    "\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 960)\n",
      "(640, 960)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cb = downsamplingChrominance(_Cb)\n",
    "Cr = downsamplingChrominance(_Cr)\n",
    "len(Cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação da DCT (Discrete Cosine Transform)\n",
    "Nessa e na próxima etapa exploramos a característica do olho humano de não ser tão bom em distinguir elementos de alta frequência em uma imagem. Para cada seção da imagem, são \"detectadas\" regiões de alta frequência de luminância ou chrominance, que então podem ser descartados. Vamos às etapas do processo da aplicação da DCT.\n",
    "\n",
    "- Para começar, dividimos a imagem em secções de 8x8 pixels.\n",
    "- Então, diminuimos cada valor em 128 (o intervalo se torna (-128, +128)). -128 é preto enquanto que +128 é branco.\n",
    "- Então, aplicamos a DCT em todas as secções 8x8 nas 3 matrizes Y, Cb e Cr.\n",
    "\n",
    "Agora, temos todos os valores das três matrizes baseadas nas constantes definidas na DCT. Repare que ainda não realizamos nenhuma compressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageSections(colorMatrix):\n",
    "    return split(colorMatrix, 8)\n",
    "\n",
    "def decreasePixelsValues(pixelSection):\n",
    "    return pixelSection - 128\n",
    "\n",
    "def runDCT(pixelSection):\n",
    "    \"\"\" Get 2D Cosine Transform of Image \"\"\"\n",
    "    return fftpack.dct(fftpack.dct(pixelSection.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "def applyDCT(colorMatrix):\n",
    "    pixelSections = getImageSections(colorMatrix)\n",
    "    # if len(pixelSections) > 30000:\n",
    "    #     print(pixelSections[30000])\n",
    "    dctSections = []\n",
    "    \n",
    "    for pixelSection in pixelSections:\n",
    "        decreased = decreasePixelsValues(pixelSection)\n",
    "        dct = runDCT(decreased)\n",
    "        dctSections.append(dct)\n",
    "\n",
    "    return dctSections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dct_sections = applyDCT(Y)\n",
    "Cb_dct_sections = applyDCT(Cb)\n",
    "Cr_dct_sections = applyDCT(Cr)\n",
    "len(Cr_dct_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantização\n",
    "Para então realizar a compreensão, passamos por cada bloco de 8x8 pixels e dividimos os valores (que já estão baseados nas constantes da DCT) pelos valores correspondes da tabela de quantização. A tabela de quantização possui valores mais altos no canto inferior direito( que correspondem as posições que possuem maior frequêcia na seção 8x8 de pixels). E então, arredondamos esses valores calculados para o inteiro mais próximo. Percebemos que muitos se tornam zeros, em especial, aqueles que possuem alta frequência. Portanto, estamos descartando dados da imagem original. Porém, são dados que nossos olhos mal conseguem notar. \n",
    "\n",
    "Podemos ter tabelas de quantização diferentes para a matriz Y e as matrizes Cb e Cr. Isso porque desejamos manter mais informções da matrix Y, então utilizamos valores menores na tabela de quantização. \n",
    "\n",
    "É nessa etapa também que é possível controlar o nível de compressão, com valores mais altos na tabela de quantização comprimindo mais a imagem, enquanto que valores menores mantém a imagem menos alterada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TABLE = np.array([[8, 6, 5, 8, 12, 20, 26, 31], \n",
    "                [6, 6, 7, 10, 13, 29, 30, 28],\n",
    "                [7, 7, 8, 12, 20, 29, 35, 28],\n",
    "                [7, 9, 11, 15, 25, 44, 40, 31],\n",
    "                [9, 11, 19, 28, 34, 55, 52, 39],\n",
    "                [12, 18, 28, 32, 41, 52, 57, 46],\n",
    "                [25, 32, 39, 44, 52, 61, 60, 51],\n",
    "                [36, 46, 48, 49, 56, 50, 52, 50]])\n",
    "\n",
    "\n",
    "CHR_TABLE = np.array([[16, 11, 10, 16, 24, 40, 51, 61], \n",
    "                [12, 12, 14, 19, 26, 58, 60, 55],\n",
    "                [14, 13, 16, 24, 40, 57, 69, 56],\n",
    "                [14, 17, 22, 29, 51, 87, 80, 62],\n",
    "                [18, 22, 37, 56, 68, 109, 103, 77],\n",
    "                [24, 35, 55, 64, 81, 104, 113, 92],\n",
    "                [49, 64, 78, 87, 103, 121, 120, 101],\n",
    "                [72, 92, 95, 98, 112, 100, 103, 99]])\n",
    "\n",
    "Y_TABLE_2 = CHR_TABLE\n",
    "\n",
    "CHR_TABLE_2 = np.array([[17, 18, 24, 47, 99, 99, 99, 99], \n",
    "                [18, 21, 26, 66, 99, 99, 99, 99],\n",
    "                [24, 26, 56, 99, 99, 99, 99, 99],\n",
    "                [47, 66, 99, 99, 99, 99, 99, 99],\n",
    "                [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "                [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "                [99, 99, 99, 99, 99, 99, 99, 99],\n",
    "                [99, 99, 99, 99, 99, 99, 99, 99]])\n",
    "\n",
    "def divide(pixelSection, table):\n",
    "    for i in range (0, pixelSection.shape[1]):\n",
    "        for j in range (0, pixelSection.shape[0]):\n",
    "            pixelSection[i][j] /= table[i][j]\n",
    "\n",
    "def runQuantization(pixelSections, table):\n",
    "    qnt_sections = []\n",
    "    for pixelSection in pixelSections:\n",
    "        divided = np.divide(pixelSection, table[0:pixelSection.shape[1], 0:pixelSection.shape[0]])\n",
    "        qnt_matrix = divided.round()\n",
    "        qnt_sections.append(qnt_matrix)\n",
    "\n",
    "    return qnt_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38400\n",
      "[[87.  1. -1. -0. -0.  0.  0.  0.]\n",
      " [-6. -1.  0. -0.  0.  0. -0. -0.]\n",
      " [-1.  1.  0.  0.  0.  0.  0. -0.]\n",
      " [ 0. -0. -0. -0.  0. -0. -0. -0.]\n",
      " [ 0. -0. -0. -0.  0. -0. -0. -0.]\n",
      " [ 0.  0.  0.  0.  0. -0.  0. -0.]\n",
      " [-0.  0. -0.  0.  0. -0. -0.  0.]\n",
      " [-0. -0.  0. -0. -0. -0.  0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "Y_qnt_sections = runQuantization(Y_dct_sections, Y_TABLE_2)\n",
    "Cb_qnt_sections = runQuantization(Cb_dct_sections, CHR_TABLE_2)\n",
    "Cr_qnt_sections = runQuantization(Cr_dct_sections, CHR_TABLE_2)\n",
    "\n",
    "print(len(Y_qnt_sections))\n",
    "print(Y_qnt_sections[30000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificação de Símbolos\n",
    "Basta agora codificar de uma forma mais \"esperta\" e suscinta os valores de cada pixel da imagem. Para cada seção de pixels, iteramos em forma de zigue zague (pois é mais provável que valores não nulos estejam no canto superior esquerdo da seção) e então aplicamos um algoritmo de \"comprimento\", que vai substituir os valores iguais por apenas um deles e o número de ocorrências. Depois, aplicamos a codificação de Huffman para gerar um código baseado na probabilidade de cada um dos valores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zigZag(matrix, rows, columns):\n",
    "    solution=[[] for i in range(rows+columns-1)]\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            sum=i+j\n",
    "            if(sum%2 ==0):\n",
    "                solution[sum].insert(0,matrix[i][j])\n",
    "            else:\n",
    "                #print(matrix)\n",
    "                solution[sum].append(matrix[i][j])\n",
    "\n",
    "    return np.concatenate(solution, axis = 0)\n",
    "\n",
    "def shrink(sectionAsArray):\n",
    "    final = []\n",
    "    previous = sectionAsArray[0]\n",
    "    counter = 0\n",
    "    for elem in sectionAsArray:\n",
    "        if elem == previous:\n",
    "            counter += 1\n",
    "        else:\n",
    "            final.append(str(int(previous)))\n",
    "            final.append(str(int(counter)))\n",
    "            previous = elem\n",
    "            counter = 1\n",
    "    \n",
    "    final.append(str(int(previous)))\n",
    "    final.append(str(int(counter)))\n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "def runLengthAlgorithm(pixelSection):\n",
    "    zz = zigZag(pixelSection, pixelSection.shape[0], pixelSection.shape[1]) # TALVEZ SEJA O CONTRARIO\n",
    "    return shrink(zz)\n",
    "\n",
    "def runHuffmanEncoding(lengthRes):\n",
    "    pass\n",
    "\n",
    "def encodeQntSections(qnts_sections):\n",
    "    finalString = ''\n",
    "    for qnt_section in qnts_sections:\n",
    "        lengthRes = runLengthAlgorithm(qnt_section)\n",
    "        lenghStr = ''\n",
    "        for elem in lengthRes:\n",
    "            lenghStr = lenghStr + elem + \" \"\n",
    "        \n",
    "        finalString = finalString + lenghStr + '\\n'\n",
    "    \n",
    "    #huffmanRes = runHuffmanEncoding(lengthRes)\n",
    "\n",
    "    return finalString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_encoded = encodeQntSections(Y_qnt_sections)\n",
    "Cb_encoded = encodeQntSections(Cb_qnt_sections)\n",
    "Cr_encoded = encodeQntSections(Cr_qnt_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto! Agora a imagem está codificada. Um exemplo de seção de 8x8 pixels pode ser visualizada abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = str(img_width) + \" \" + str(img_height) + \"\\n\"\n",
    "\n",
    "file = open(\"my_awesome_img.lpeg\", \"w\")\n",
    "file.write(header + Y_encoded + Cb_encoded + Cr_encoded)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomprimindo a Imagem\n",
    "O processo de descompressão é simples, basta realizar o processo de compressão na ordem inversa. As etapas estão listadas a seguir:\n",
    "1. Lemos o arquivo comprimido, e então separamos as secções de pixels referentes a cada matrix Y, Cb e Cr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileAndGetPixelSections(filename):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    header = lines[0].split()\n",
    "    \n",
    "    img_width = int(header[0]) \n",
    "    img_height = int(header[1])\n",
    "    Y_pixelSections = []\n",
    "    Cb_pixelSections = []\n",
    "    Cr_pixelSections = []\n",
    "\n",
    "    Y_lines_num = math.ceil(img_width/8) * math.ceil(img_height/8)\n",
    "    \n",
    "    Cb_Cr_width = math.ceil(img_width/2)\n",
    "    Cb_Cr_height = math.ceil(img_height/2)\n",
    "\n",
    "    Cb_Cr_lines_num = math.ceil(Cb_Cr_width/8) * math.ceil(Cb_Cr_height/8)\n",
    "\n",
    "    counter = 1\n",
    "    for line in lines[1:]:\n",
    "        res = line.split()\n",
    "        if counter <= Y_lines_num:\n",
    "            Y_pixelSections.append(res)\n",
    "            counter += 1\n",
    "            continue\n",
    "        \n",
    "        if counter > Y_lines_num and counter <= Y_lines_num + Cb_Cr_lines_num:\n",
    "            Cb_pixelSections.append(res)\n",
    "            counter += 1\n",
    "            continue\n",
    "        \n",
    "        Cr_pixelSections.append(res)\n",
    "        counter += 1\n",
    "\n",
    "    return img_width, img_height, Y_pixelSections, Cb_pixelSections, Cr_pixelSections\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "1280\n",
      "38400\n",
      "9600\n",
      "9600\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height, Y_pixelSections, Cb_pixelSections, Cr_pixelSections = readFileAndGetPixelSections(\"my_awesome_img.lpeg\")\n",
    "print(img_width)\n",
    "print(img_height)\n",
    "print(len(Y_pixelSections))\n",
    "print(len(Cb_pixelSections))\n",
    "print(len(Cr_pixelSections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Para cada secção, decodificamos utilizando o decodificador de huffman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeFromHuffman(encodedPixelSection):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Então, reconstruimos a matriz 8x8 de acordo com a decodificação do algoritmo de comprimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(pixelSectionLength):\n",
    "    final = []\n",
    "    for i in range(0, len(pixelSectionLength), 2):\n",
    "        for j in range(0, int(pixelSectionLength[i+1])):\n",
    "            final.append(int(pixelSectionLength[i]))\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def getPixelSectionMatrix(pixelSectionLength, shape):\n",
    "    expanded = expand(pixelSectionLength)   ## BACK WITH THIS LINE\n",
    "    matrix = np.zeros(shape)\n",
    "    solution=[[] for i in range(shape[0] + shape[1] - 1)]\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            sum=i+j\n",
    "            if(sum%2 == 0):\n",
    "                solution[sum].insert(0,matrix[i][j])\n",
    "            else:\n",
    "                solution[sum].append(matrix[i][j])\n",
    "\n",
    "    counter = 0\n",
    "    for diag in solution:\n",
    "        for idx in range(len(diag)):\n",
    "            diag[idx] = expanded[counter]\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            sum = i+j\n",
    "            if(sum%2 == 0):\n",
    "                matrix[i][j] = solution[i+j][len(solution[i+j]) - 1]\n",
    "                solution[i+j].pop()\n",
    "            else:\n",
    "                matrix[i][j] = solution[i+j][0]\n",
    "                solution[i+j].pop(0)\n",
    "\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def getDecodedPixelSections(pixelSections):\n",
    "    final = []\n",
    "    for pixelSection in pixelSections:\n",
    "        final.append(getPixelSectionMatrix(pixelSection, (8,8))) # TEM Q CALCULAR O SHAPE(NEM SMP 8x8)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_decoded = getDecodedPixelSections(Y_pixelSections)\n",
    "Cb_decoded = getDecodedPixelSections(Cb_pixelSections)\n",
    "Cr_decoded = getDecodedPixelSections(Cr_pixelSections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Multiplicamos o resultado de cada seção pela tabela de quantização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplyByQuantization(pixelSections, table):\n",
    "    final = []\n",
    "    for pixelSection in pixelSections:\n",
    "        multiplied = np.multiply(pixelSection, table)\n",
    "        final.append(multiplied)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1392.,   11.,  -10.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [ -72.,  -12.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [ -14.,   13.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_multiplied = multiplyByQuantization(Y_decoded, Y_TABLE_2) # MUST BE THE SAME AS THE TABLE OF THE COMPRESSION\n",
    "Cb_multiplied = multiplyByQuantization(Cb_decoded, CHR_TABLE_2) # MUST BE THE SAME AS THE TABLE OF THE COMPRESSION\n",
    "Cr_multiplied = multiplyByQuantization(Cr_decoded, CHR_TABLE_2) # MUST BE THE SAME AS THE TABLE OF THE COMPRESSION\n",
    "Y_multiplied[30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Aplicamos a DCT inversa em cada seção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increasePixelsValues(pixelSection):\n",
    "    return (pixelSection + 128).round()\n",
    "\n",
    "def applyInverseDCT(pixelSections):\n",
    "    final = []\n",
    "    for pixelSection in pixelSections:\n",
    "        idct = get_2d_idct(pixelSection)\n",
    "        increased = increasePixelsValues(idct)\n",
    "        final.append(increased)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_inversed = applyInverseDCT(Y_multiplied)\n",
    "Cb_inversed = applyInverseDCT(Cb_multiplied)\n",
    "Cr_inversed = applyInverseDCT(Cr_multiplied)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
