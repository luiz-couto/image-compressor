{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(matrix, num):\n",
    "    sections = []\n",
    "    for i in range(0, matrix.shape[0], num):\n",
    "        for j in range(0, matrix.shape[1], num):\n",
    "            section = matrix[i:i+num, j:j+num]\n",
    "            sections.append(section)\n",
    "\n",
    "    return sections\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[128., 123., 122., 129.],\n",
       "       [120., 120., 183., 170.],\n",
       "       [133.,  86., 141., 140.],\n",
       "       [138., 130., 128., 128.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"fish.png\")\n",
    "YCbCr_img = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "Y, Cr, Cb = cv2.split(YCbCr_img)\n",
    "\n",
    "img_width = img.shape[1]\n",
    "img_height = img.shape[0]\n",
    "\n",
    "# quantization matrix\n",
    "QM = np.matrix([[16, 11, 10, 16, 24, 40, 51], \n",
    "                [12, 12, 14, 19, 26, 58, 60],\n",
    "                [14, 13, 16, 24, 40, 57, 69],\n",
    "                [14, 17, 22, 29, 51, 87, 80],\n",
    "                [18, 22, 37, 56, 68, 109, 103],\n",
    "                [24, 35, 55, 64, 81, 104, 113],\n",
    "                [49, 64, 78, 87, 103, 121, 120],\n",
    "                [72, 92, 95, 98, 112, 100, 103]])\n",
    "\n",
    "\n",
    "#plt.imshow(Cr)\n",
    "# print(Y)\n",
    "# print(\"------------\")\n",
    "# print(Y[0:2, 0:2])\n",
    "# print(\"------------\")\n",
    "# print(Y[0:2, 2:4])\n",
    "# print(\"------------\")\n",
    "# print(Y[0:2, 4:6])\n",
    "# print(\"------------\")\n",
    "# print(Y[0:2, 6:8])\n",
    "\n",
    "\n",
    "# print(\"------------\")\n",
    "# print(Y[2:4, 0:2])\n",
    "# print(\"------------\")\n",
    "# print(Y[2:4, 2:4])\n",
    "# print(\"------------\")\n",
    "# print(Y[2:4, 4:6])\n",
    "# print(\"------------\")\n",
    "# print(Y[2:4, 6:8])\n",
    "\n",
    "\n",
    "pixelSections = split(Cb,2)\n",
    "\n",
    "new_matrix = np.zeros((round(img_height/2), round(img_width/2)))\n",
    "counter = 0\n",
    "for i in range(0, round(img_width/2)):\n",
    "    for j in range(0, round(img_height/2)):\n",
    "        pixelSection = pixelSections[counter]\n",
    "        new_matrix[i][j] = int(round(pixelSection.sum() / 4))\n",
    "        counter += 1\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressor de Imagens\n",
    "O método utilizado nesse trabalho seguirá as diretrizes do método JPEG para imagens cromáticas (com cores). Será um método com perdas. Esse método pode ser dividido em 5 etapas:\n",
    "1. Conversão do espaço de cores\n",
    "2. Downsampling da chrominance\n",
    "3. Aplicação da DCT\n",
    "4. Quantização\n",
    "5. Codificação de Símbolos utilzando Huffman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão do Espaço de Cores\n",
    "Cada pixel de uma imagem colorida é dividida em 3 valores: R(red), G(green), B(blue). No método JPEG, esses valores são convertidos para Y(Luminance) Cb(blue chrominance) Cr(red chrominance). Isso é feito pois o olho humano é capaz de detectar brilho e luminâcia melhor do que detectar cores. Ao separar as cores desse modo, podemos, no próximo passo (Chrominance Downsampling) descartar algumas informações de Cb e Cr, de forma a já diminuir o tamanho necessário para cada pixel enquanto mantemos Y(que se refere ao brilho) intacto. Nesse caso, exploramos a redundânica psicovisual ao remover informações que não são facilmente detectadas, e mantendo aquelas que conseguimos perceber melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRGBToYCbCr(imgInRGB):\n",
    "    YCbCr_img = cv2.cvtColor(imgInRGB, cv2.COLOR_RGB2YCrCb)\n",
    "    Y, Cr, Cb = cv2.split(YCbCr_img)\n",
    "    return Y, Cb, Cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrominance Downsampling\n",
    "Para realizar o downsampling, vamos pegar as matrizes referentes à Cb e Cr e então dividi-las em blocos de 2x2 pixels. Para cada bloco, fazemos a média dos valores dos 4 pixels, e então substituimos o bloco inteiro por apenas o valor calculado. Eliminamos, assim, grande parte da informação que nossos olhos mal conseguem perceber. Repare que desse modo diminuimos as matrizes Cb e Cr em 1/4 da imagem da original. Porém, mantivemos a matriz Y no seu tamanho original, que possui os valores relacionados ao brilho e luminânica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsamplingChrominance(chrominanceMatrix):\n",
    "    pixelSections = split(chrominanceMatrix, 2)\n",
    "\n",
    "    new_matrix = np.zeros((round(img_height/2), round(img_width/2)))\n",
    "    counter = 0\n",
    "    for i in range(0, round(img_width/2)):\n",
    "        for j in range(0, round(img_height/2)):\n",
    "            pixelSection = pixelSections[counter]\n",
    "            new_matrix[i][j] = int(round(pixelSection.sum() / 4))\n",
    "            counter += 1\n",
    "\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação da DCT (Discrete Cosine Transform)\n",
    "Nessa e na próxima etapa exploramos a característica do olho humano de não ser tão bom em distinguir elementos de alta frequência em uma imagem. Para cada seção da imagem, são \"detectadas\" regiões de alta frequência de luminância ou chrominance, que então podem ser descartados. Vamos às etapas do processo da aplicação da DCT.\n",
    "\n",
    "- Para começar, dividimos a imagem em secções de 8x8 pixels.\n",
    "- Então, diminuimos cada valor em 128 (o intervalo se torna (-128, +128)). -128 é preto enquanto que +128 é branco.\n",
    "- Então, aplicamos a DCT em todas as secções 8x8 nas 3 matrizes Y, Cb e Cr.\n",
    "\n",
    "Agora, temos todos os valores das três matrizes baseadas nas constantes definidas na DCT. Repare que ainda não realizamos nenhuma compressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageSections(colorMatrix):\n",
    "    pass\n",
    "\n",
    "def decreasePixelsValues(pixelSection):\n",
    "    pass\n",
    "\n",
    "def runDCT(pixelSection):\n",
    "    pass\n",
    "\n",
    "def applyDCT(colorMatrix):\n",
    "    pixelSections = getImageSections(colorMatrix)\n",
    "    \n",
    "    for pixelSection in pixelSections:\n",
    "        decreasePixelsValues(pixelSection)\n",
    "\n",
    "    for pixelSection in pixelSections:\n",
    "        runDCT(pixelSection)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantização\n",
    "Para então realizar a compreensão, passamos por cada bloco de 8x8 pixels e dividimos os valores (que já estão baseados nas constantes da DCT) pelos valores correspondes da tabela de quantização. A tabela de quantização possui valores mais altos no canto inferior direito( que correspondem as posições que possuem maior frequêcia na seção 8x8 de pixels). E então, arredondamos esses valores calculados para o inteiro mais próximo. Percebemos que muitos se tornam zeros, em especial, aqueles que possuem alta frequência. Portanto, estamos descartando dados da imagem original. Porém, são dados que nossos olhos mal conseguem notar. \n",
    "\n",
    "Podemos ter tabelas de quantização diferentes para a matriz Y e as matrizes Cb e Cr. Isso porque desejamos manter mais informções da matrix Y, então utilizamos valores menores na tabela de quantização. \n",
    "\n",
    "É nessa etapa também que é possível controlar o nível de compressão, com valores mais altos na tabela de quantização comprimindo mais a imagem, enquanto que valores menores mantém a imagem menos alterada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TABLE = []\n",
    "CHR_TABLE = []\n",
    "\n",
    "def runQuantization(pixelSection, table):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificação de Símbolos\n",
    "Basta agora codificar de uma forma mais \"esperta\" e suscinta os valores de cada pixel da imagem. Para cada seção de pixels, iteramos em forma de zigue zague (pois é mais provável que valores não nulos estejam no canto superior esquerdo da seção) e então aplicamos um algoritmo de \"comprimento\", que vai substituir os valores iguais por apenas um deles e o número de ocorrências. Depois, aplicamos a codificação de Huffman para gerar um código baseado na probabilidade de cada um dos valores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLengthAlgorithm(pixelSection):\n",
    "    pass\n",
    "\n",
    "def runHuffmanEncoding(lengthRes):\n",
    "    pass\n",
    "\n",
    "def encodePixelSection(pixelSection):\n",
    "    lengthRes = runLengthAlgorithm(pixelSection)\n",
    "    huffmanRes = runHuffmanEncoding(lengthRes)\n",
    "    return huffmanRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto! Agora a imagem está codificada. Um exemplo de seção de 8x8 pixels pode ser visualizada abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomprimindo a Imagem\n",
    "O processo de descompressão é simples, basta realizar o processo de compressão na ordem inversa. As etapas estão listadas a seguir:\n",
    "1. Lemos o arquivo comprimido, e então separamos as secções de pixels referentes a cada matrix Y, Cb e Cr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileAndGetPixelSections(filename):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Para cada secção, decodificamos utilizando o decodificador de huffman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeFromHuffman(encodedPixelSection):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Então, reconstruimos a matriz 8x8 de acordo com a decodificação do algoritmo de comprimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPixelSectionMatrix(pixelSectionLength):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Multiplicamos o resultado de cada seção pela tabela de quantização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplyByQuantization(pixelSection, table):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Aplicamos a DCT inversa em cada seção."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
